---
layout: cn_post
title: n-gram语言模型，hmm模型，统一框架
categories: [语言, 2013]
tags: [语言, 分词, 模型, ngram, hmm]
---

中文分词，音字转换，拼写检查，均可以用n-gram语言模型和hmm模型来统一处理。
 

语言模型

利用语言模型，可以确定哪个词序列的可能性更大，或者给定若干个词，可以预测下一个最可能出现的词语。举个音字转换的例子来说，输入拼音串为nixianzaiganshenme，对应的输出可以有多种形式，如你现在干什么、你西安再赶什么、等等，那么到底哪个才是正确的转换结果呢，利用语言模型，我们知道前者的概率大于后者，因此转换成前者在多数情况下比较合理。再举一个机器翻译的例子，给定一个汉语句子为李明正在家里看电视，可以翻译为Li Ming is watching TV at home、Li Ming at home is watching TV、等等，同样根据语言模型，我们知道前者的概率大于后者，所以翻译成前者比较合理。


n-gram语言模型

n-gram模型的概念

n-gram模型也称为n-1阶马尔科夫模型，它有一个有限历史假设：当前词的出现概率仅仅与前面n-1个词相关。因此可以近似为：

p(S)=p(w1,w2,w3,w4,w5,…,wn) =p(w1)p(w2|w1)p(w3|w1,w2)...p(wn|w1,w2,...,wn-1)//链规则 

为方便计算，log(p1*p2*p3*p4) = log(p1) + log(p2) + log(p3) + log(p4)


当n取1、2、3时，n-gram模型分别称为unigram、bigram和trigram语言模型。n-gram模型的参数就是条件概率。

假设词表的大小为100,000，那么n-gram模型的参数数量为。

n越大，模型越准确，也越复杂，需要的计算量越大。最常用的是bigram，其次是unigram和trigram，n取≥4的情况较少。

n-gram模型的参数估计

模型的参数估计也称为模型的训练，一般采用最大似然估计（Maximum Likelihood Estimation，MLE）的方法对模型的参数进行估计： 

(3) 

C(X)表示X在训练语料中出现的次数，训练语料的规模越大，参数估计的结果越可靠。但即使训练数据的规模很大，如若干GB，还是会有很多语言现象在训练语料中没有出现过，这就会导致很多参数（某n元对的概率）为0。这种问题也被称为数据稀疏（Data Sparseness），解决数据稀疏问题可以通过数据平滑（Data Smoothing）技术来解决。它们的基本思想是“降低已出现n-gram条件概率分布，以使未出现的n-gram条件概率分布非零”，且经数据平滑后一定保证概率和为1，

 加法平滑 

基本思想是为避免零概率问题，将每个n元对得出现次数加上一个常数δ（0<δ≤1）： 

(4)

{% highlight vim %}
-------------------------------------------------------

输入:

M = 每列最多词语数;

T= 拼音序列中的拼音数;

w[1:T,1:M]=词网格;

Bigram(x,y) = 任意两个词语对应的Bigram概率;

输出:

概率最大词语序列TARGET[1:T];

初始化:

w[0,0] = “”;

Prob[0,0] = 1

TARGET[1:T]={“”};

解码:

For t = 1 : T Do

For i = 1 : M Do

MaxProb = 0;

Index = 0;

For j = 1 : M Do

ThisProb = Prob[t-1,j]*Bigram(w[t-1,j],w[t,i]);

If ThisProb > MaxProb

MaxProb = ThisProb;

Index = j;

End If

End For

Prob[t,i] = MaxProb;

Ptr[t,i] = j;

End For

End For

终止:

Prob = ; // 最后一列中最大的概率

TARGET [T]= 最后一列取得最大概率的那个词语w[T,k];

回溯:

t = T;

While t > 0 Do

s = t – Len(TARGET [T]);

TARGET[s] = w[s,Ptr[t, TARGET[t]]]

t = s;

End While

-------------------------------------------------------
{% endhighlight %}

上面是最基本的解码算法，实际工程中，需要考虑的因素还有很多，比如音节切分歧义、大规模解码算法的剪枝、更高阶N元模型的解码算法等。
 

中文纠错

中文的纠错主要是针对词，相当于英语的phrase。面对word的纠错无法直接应用于中文。中文的纠错会根据词到拼音的转换，得到具有同样拼音的其 他的词或具有前后鼻音的会混淆的拼音对应的其他的词。这里切得的词可以基于分词算法也可以基于n-gram来得到词，并需要分析词之间的关联或单个词的出 现概率等 

一般需要挑选和其他词关联明显小，或给定词本身出现概率相比同拼音其他词出现概率小的进行所谓的纠正。为了做到这点，需要有对应的转换词库以及对应的同音 词的出现频率以及和其他词的co-occurrence等计算所得的概率。这里也可以应用同样适用于英文纠错的near duplicated的query log进行推荐。
 

 参考

1. 微软拼音输入法团队博客: 语言模型的基本概念，N-gram语言模型的训练方法，N元语言模型的解码算法。

2. 我爱公开课：斯坦福大学自然语言处理第四课“语言模型（Language Modeling）”

3. 我爱公开课：斯坦福大学自然语言处理第五课“拼写纠错（Spelling Correction）”

4. matrix67：漫话中文自动分词和语义识别

5. norvig：拼写检查器

6. mindhacks：数学之美番外篇：平凡而又神奇的贝叶斯方法


