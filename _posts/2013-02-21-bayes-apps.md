---
layout: cn_post
title: 贝叶斯方法
categories: [语言]
tags: [语言, 贝叶斯, ngram, hmm, viterbi]
---

自然语言处理中有很多概念，在初学时因为不理解各种缘由，很容易云里雾里，其实本质上都是一回事。

经典的贝叶斯方法

n-gram语言模型

hmm模型

噪音信道模型

动态规划的viterbi算法

 

贝叶斯方法

所谓的贝叶斯方法源于他生前为解决一个“逆概”问题写的一篇文章，而这篇文章是在他死后才由他的一位朋友发表出来的。在贝叶斯写这篇文章之前，人们已经能够计算“正向概率”，如“假设袋子里面有N个白球，M个黑球，你伸手进去摸一把，摸出黑球的概率是多大”。而一个自然而然的问题是反过来：“如果我们事先并不知道袋子里面黑白球的比例，而是闭着眼睛摸出一个（或好几个）球，观察这些取出来的球的颜色之后，那么我们可以就此对袋子里面的黑白球的比例作出什么样的推测”。这个问题，就是所谓的逆概问题。

P(h | D) = P(h) * P(D | h) / P(D)

P(h | D) ∝ P(h) * P(D | h)

这个式子的抽象含义是：对于给定观测数据，一个猜测是好是坏，取决于“这个猜测本身独立的可能性大小（先验概率，Prior ）”和“这个猜测生成我们观测到的数据的可能性大小”（似然，Likelihood ）的乘积。

中文分词

我们用贝叶斯公式来形式化地描述这个问题，令 X 为字串（句子），Y 为词串（一种特定的分词假设）。我们就是需要寻找使得 P(Y|X) 最大的 Y ，使用一次贝叶斯可得：

P(Y|X) ∝ P(Y)*P(X|Y)

用自然语言来说就是 这种分词方式（词串）的可能性 乘以 这个词串生成我们的句子的可能性。我们进一步容易看到：可以近似地将 P(X|Y) 看作是恒等于 1 的，因为任意假想的一种分词方式之下生成我们的句子总是精准地生成的（只需把分词之间的分界符号扔掉即可）。于是，我们就变成了去最大化 P(Y) ，也就是寻找一种分词使得这个词串（句子）的概率最大化。而如何计算一个词串：

W1, W2, W3, W4 ..

的可能性呢？我们知道，根据联合概率的公式展开：P(W1, W2, W3, W4 ..) = P(W1) * P(W2|W1) * P(W3|W2, W1) * P(W4|W1,W2,W3) * .. 于是我们可以通过一系列的条件概率（右式）的乘积来求整个联合概率。然后可以使用n-gram语言模型简化计算。

n-gram语言模型

在实际应用中，我们经常需要解决这样一类问题：如何计算一个句子的概率？如： 

    机器翻译：P(highwinds tonite) > P(large winds tonite)

    拼写纠错：P(about fifteen minutes from) > P(about fifteen minuets from)

    语音识别：P(I saw a van) >> P(eyes awe of an)

    音字转换：P(你现在干什么|nixianzaiganshenme) > P(你西安在干什么|nixianzaiganshenme)

    自动文摘、问答系统、... ...

以上问题的形式化表示如下：

p(S)=p(w1,w2,w3,w4,w5,…,wn)=p(w1)p(w2|w1)p(w3|w1,w2)...p(wn|w1,w2,...,wn-1)//链规则

p(S)被称为语言模型，即用来计算一个句子概率的模型。

那么，如何计算p(wi|w1,w2,...,wi-1)呢？最简单、直接的方法是直接计数做除法，如下：

p(wi|w1,w2,...,wi-1) = p(w1,w2,...,wi-1,wi) / p(w1,w2,...,wi-1)

但是，这里面临两个重要的问题：数据稀疏严重；参数空间过大，无法实用。

基于马尔科夫假设（Markov Assumption）：下一个词的出现仅依赖于它前面的一个或几个词。

    假设下一个词的出现依赖它前面的一个词，则有：

p(S)=p(w1)p(w2|w1)p(w3|w1,w2)...p(wn|w1,w2,...,wn-1)

=p(w1)p(w2|w1)p(w3|w2)...p(wn|wn-1) // bigram

    假设下一个词的出现依赖它前面的两个词，则有：

p(S)=p(w1)p(w2|w1)p(w3|w1,w2)...p(wn|w1,w2,...,wn-1)

=p(w1)p(w2|w1)p(w3|w1,w2)...p(wn|wn-1,wn-2) // trigram 

那么，我们在面临实际问题时，如何选择依赖词的个数，即n。

    更大的n：对下一个词出现的约束信息更多，具有更大的辨别力；

    更小的n：在训练语料库中出现的次数更多，具有更可靠的统计信息，具有更高的可靠性。

理论上，n越大越好，经验上，trigram用的最多，尽管如此，原则上，能用bigram解决，绝不使用trigram。

 hmm模型

吴军在数学之美系列里面介绍的隐马可夫模型（HMM）就是一个简单的层级贝叶斯模型：

那么怎么根据接收到的信息来推测说话者想表达的意思呢？我们可以利用叫做“隐含马尔可夫模型”（Hidden Markov Model）来解决这些问题。以语音识别为例，当我们观测到语音信号 o1,o2,o3 时，我们要根据这组信号推测出发送的句子 s1,s2,s3。显然，我们应该在所有可能的句子中找最有可能性的一个。用数学语言来描述，就是在已知 o1,o2,o3,…的情况下，求使得条件概率 P (s1,s2,s3,…|o1,o2,o3….) 达到最大值的那个句子 s1,s2,s3,…

吴军的文章中这里省掉没说的是，s1, s2, s3, .. 这个句子的生成概率同时又取决于一组参数，这组参数决定了 s1, s2, s3, .. 这个马可夫链的先验生成概率。如果我们将这组参数记为 λ ，我们实际上要求的是：P(S|O, λ) （其中 O 表示 o1,o2,o3,.. ，S表示 s1,s2,s3,..）

当然，上面的概率不容易直接求出，于是我们可以间接地计算它。利用贝叶斯公式并且省掉一个常数项，可以把上述公式等价变换成

P(o1,o2,o3,…|s1,s2,s3….) * P(s1,s2,s3,…)

其中

P(o1,o2,o3,…|s1,s2,s3….) 表示某句话 s1,s2,s3…被读成 o1,o2,o3,…的可能性, 而 P(s1,s2,s3,…) 表示字串 s1,s2,s3,…本身能够成为一个合乎情理的句子的可能性，所以这个公式的意义是用发送信号为 s1,s2,s3…这个数列的可能性乘以 s1,s2,s3.. 本身可以一个句子的可能性，得出概率。

这里，s1,s2,s3…本身可以一个句子的可能性其实就取决于参数 λ ，也就是语言模型。所以简而言之就是发出的语音信号取决于背后实际想发出的句子，而背后实际想发出的句子本身的独立先验概率又取决于语言模型。

 噪音信道模型


